# üìà PredicTick - Financial Market Prediction Framework

**Author:** Arturo Mendoza ([arturo.amb89@gmail.com](mailto:arturo.amb89@gmail.com))

---

## üöÄ Overview

PredicTick is a modular framework for forecasting market direction ‚Äî Down, Neutral, or Up ‚Äî based on historical price action and technical indicators.

The name blends "Predict" and "Tick", reflecting its core mission: predicting the next market tick with precision, speed, and intelligence.

PredicTick combines modern machine learning with financial expertise to support daily predictions, training pipelines, backtesting simulations, and dashboards.

It includes:

* **Boosting models** (`XGBoost`) optimized via `Optuna` for multiclass classification.
* **Centralized configuration** using `ParameterLoader` for full control and reproducibility.
* **Technical feature engineering including** (RSI, MACD, Bollinger Bands, Stochastic RSI, and more).
* **Backtesting engine for historical evaluation** of prediction strategies.
* **Interactive dashboards** built with `Streamlit` and `Optuna` visualizations.

<p align="center"><img src="images/logo-1.png" alt="Logo" style="width: 50%; height: auto;"></p>

---

## üß± Project Structure

```bash
root/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ backtesting/      # Historical simulations
‚îÇ   ‚îú‚îÄ‚îÄ dashboard/        # Interactive dashboard
‚îÇ   ‚îú‚îÄ‚îÄ evaluation/       # Performance evaluation
‚îÇ   ‚îú‚îÄ‚îÄ market_data/      # Market data downloading and updating
‚îÇ   ‚îú‚îÄ‚îÄ prediction/       # Daily predictions
‚îÇ   ‚îú‚îÄ‚îÄ training/         # Model training
‚îÇ   ‚îî‚îÄ‚îÄ utils/            # Global parameters and utilities
‚îú‚îÄ‚îÄ config/               # Symbol lists and parameter files
‚îú‚îÄ‚îÄ data/                 # Data artifacts and models
‚îú‚îÄ‚îÄ images/               # App logo and other project images
‚îú‚îÄ‚îÄ README.md             # Project documentation
‚îú‚îÄ‚îÄ requirements-dev.txt  # Project dev dependencies
‚îú‚îÄ‚îÄ requirements.txt      # Project prod dependencies
‚îú‚îÄ‚îÄ envtool.sh            # Project setup and cleaning script
‚îî‚îÄ‚îÄ run_tasks.sh          # Task automation script
```

---

## ‚öôÔ∏è Requirements & Setup

> **Minimum Python version required: `3.10`**
> The framework uses advanced features such as `from __future__ import annotations` and enhanced type hinting that require Python ‚â•3.10.
> If multiple versions are installed, ensure the virtual environment uses Python 3.10 or newer.

To set up the environment:

```bash
bash envtool.sh install
```

This script will:

* Create a Python virtual environment `.venv` if missing.
* Upgrade `pip`.
* Install dependencies from `requirements.txt` and `requirements-dev.txt`.

### üì¶ Dependencies

> **Note:** For the full and up-to-date list of dependencies (including exact versions), please refer to `requirements.txt` and `requirements-dev.txt`.

**Main dependencies:**

* `pandas`, `numpy`, `scikit-learn`, `xgboost`, `imbalanced-learn`
* `optuna`, `ta`, `pandas_market_calendars`
* `yfinance`, `streamlit`, `holidays`
* `joblib`, `matplotlib`, `seaborn`, `plotly`
* Google API & environment: `google-api-python-client`, `google-auth-httplib2`, `google-auth-oauthlib`, `python-dotenv`

**Development tools:**

* `black`, `isort`, `bandit`, `pylint`, `autoflake`, `pydocstringformatter`, `coverage`, `pytest`

---

## üß† Centralized Configuration

All pipeline components use a centralized `ParameterLoader` object that handles:

* Symbols for training/prediction (`training_symbols`, `correlative_symbols`)
* Thresholds and indicator windows (e.g. RSI, MACD)
* Paths to models, scalers, plots, and JSON data
* Market timezone and FED event days
* Cutoff date for training/evaluation

This ensures **consistency, maintainability and reproducibility** across modules.

---

## üõ†Ô∏è Main Workflows

Use the `run_tasks.sh` automation script to launch any task:

### 1. üì• Update Market Data

```bash
bash run_tasks.sh update
```

### 2. üß† Train Model

```bash
bash run_tasks.sh train
```

### 3. üìä Evaluate Model

```bash
bash run_tasks.sh evaluate
```

### 4. üîÆ Daily Prediction

```bash
bash run_tasks.sh predict
```

### 5. üï∞Ô∏è Backtesting

```bash
bash run_tasks.sh backtest
```

### 6. üìà Launch Dashboard

```bash
bash run_tasks.sh dashboard
```

### 7. üöÄ Full Pipeline (Update ‚Üí Train ‚Üí Evaluate ‚Üí Predict)

```bash
bash run_tasks.sh all
```

---

## üîÄ Modeling Strategy

This framework uses a **Multi-active model** approach:

* A model is trained using multiple symbols as input, leveraging cross-symbol features such as correlation with benchmark assets (e.g., SPY).
* This allows for shared learning across assets, improving generalization and efficiency.

---

## üß† Prediction Logic

* **Multiclass classification (3 classes):**

  * `‚Üì Down` (target = 0)
  * `‚Üí Neutral` (target = 1)
  * `‚Üë Up` (target = 2)

* **Input features include:**

  * `return_1h`, `volatility_3h`, `momentum_3h`, `rsi`, `macd`, `volume`, `bb_width`
  * `stoch_rsi`, `obv`, `atr`, `williams_r`, `hour`, `day_of_week`, `is_fed_day`, `is_before_holiday`
  * Cross-asset features: `spread_vs_SPY`, `corr_5d_SPY`

* **Model:** `XGBoostClassifier` with `multi:softprob` objective

* **Optimization:** `Optuna` + `TimeSeriesSplit` + `RandomUnderSampler` for class balance

---

## üìä Example Outputs

* ‚úÖ Confusion matrix (normalized %)
* ‚úÖ F1-score per class (bar plot)
* ‚úÖ Expected return score for best symbol
* ‚úÖ Prediction logs:

```text
üü¢ SUCC | Best Symbol for 2024-06-20: AAPL ‚Üí UP (Score: 1.245)
```

---

## üìÅ Logging Directory:

All logs printed to the console during execution are also stored persistently in the logs/ directory. This includes messages from data updates, training runs, predictions, and backtests. The log files are timestamped and named accordingly to support debugging, auditing, and monitoring workflows.

---

## üìà Optuna Dashboard

An interactive Streamlit-based dashboard allows you to inspect the hyperparameter optimization results:

```bash
bash run_tasks.sh dashboard
```

Features:

* Optimization history
* Parallel coordinate plots
* Hyperparameter importance
* Slice plots per trial

---

## ‚è∞ Timezone & Calendars

* All dates are converted and processed in **America/New\_York** timezone.
* FED event days and US holidays are injected into the feature set.
* Training cutoff date is dynamically defined in `ParameterLoader`.

---

## ü™™ Data Integrity & Artifacts

* ‚úÖ Market data is validated via `validate_market_data()` on each update.
* ‚úÖ Artifacts (models, scalers, Optuna studies) are saved with timestamps for reproducibility.
* ‚úÖ All critical components are testable and modular.

---

## üìå Naming Conventions

* ‚úÖ Classes: `PascalCase`
* ‚úÖ Functions & variables: `snake_case` (PEP8)
* ‚úÖ Constants: `ALL_CAPS_SNAKE`
* ‚úÖ No usage of camelCase
* ‚úÖ Linting tools: `black`, `pylint`, `isort`, `bandit`, `autoflake`

---

## ‚ö†Ô∏è Initial Setup Required

For the project to operate correctly, you will need to create and populate certain configuration files. Please ensure the following are set up:

* **`.env` file:** Create this file in the root directory. It is used for vulnerable variables. (Note: `envtool.sh` does not automatically create this file.)

* **Configuration Directory and Files:**

  * Create a directory named `config` in the root of the project.
  * Inside `config/`, create `symbols.json`. This file should define the symbols for training and prediction.
  * Inside `config/`, create `symbols_invalid.json`. This file should list any symbols to be excluded.
  * Create a subdirectory `gcp` inside `config/` (i.e., `config/gcp/`).
  * Inside `config/gcp/`, the `credentials.json` file must be placed (explained later in detail under 'How to Generate credentials.json'). This file is needed for Google Cloud Platform interactions, such as uploading or downloading artifacts from Google Drive.

* **Example Configuration Files:**

Below are sample contents for required JSON configuration files:

  * config/symbols.json
```bash
{
  "training": ["AAPL", "BTC-USD", "EURUSD=X", "VOO", "GLD"],
  "correlative": ["VOO", "GLD"],
  "prediction_groups": [
    {
      "name": "group-1",
      "symbols": ["BTC-USD"]
    },
    {
      "name": "group-2",
      "symbols": ["AAPL", "VOO", "GLD"]
    }
  ]
}
```

  * config/symbols\_invalid.json
```bash
["TSLA"]
```

  * .env
```bash
GDRIVE_FOLDER_ID=xxx  # Folder ID in Google Drive where artifacts will be uploaded/downloaded
```

### üîê How to Generate `credentials.json`

To use Google Drive with this project (for uploading or downloading models and artifacts), follow these steps:

1. Go to the [Google Cloud Console](https://console.cloud.google.com/).
2. Create a new project (or select an existing one).
3. Enable the **Google Drive API** for the project.
4. Create credentials:
   * Choose **OAuth 2.0 Client ID** for interactive use **or**
   * Choose **Service Account** for headless/scripted access.
5. Download the resulting `credentials.json` file.
6. Save it in the path: `config/gcp/credentials.json`.

This enables secure, authenticated access to Google Drive resources from within the PredicTick framework.

## ü§ù AI GUIDE

[Open the AI Guide](AI_GUIDE.md)

---

## ü§ù Contributions

Contributions are welcome!

Please follow the existing project structure and coding standards.
To propose improvements or new features (e.g. model variants, indicators, dashboards), open a PR or issue.

---

> *‚ÄúThe future belongs to those who anticipate it.‚Äù*
